{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "606q4MQJZz2U"
   },
   "source": [
    "# Model Selection for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp1ELm57Zz2a"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1725951345762,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "HDmggAMy91S3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## adapt this directory to your needs\n",
    "base_dir = '../'\n",
    "notebook_dir = os.path.join(base_dir, 'Exercise')\n",
    "data_dir = os.path.join(base_dir, 'DataSets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 4639,
     "status": "ok",
     "timestamp": 1725951415531,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "vXCty5i0Zz2b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyMLaux import show_img_data, evaluate_classification_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c9KhDatZz2o"
   },
   "source": [
    "## Load Simple Digit Recognition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 1413,
     "status": "ok",
     "timestamp": 1725951420044,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "D6TPTi1qZz2o"
   },
   "outputs": [],
   "source": [
    "dig_train_raw = pd.read_csv(os.path.join(data_dir,'Digits_training.csv'), sep=',')\n",
    "dig_train = {'data': np.array(dig_train_raw.iloc[:, :-1]),\n",
    "             'target': np.array(dig_train_raw.iloc[:, -1]),\n",
    "             'feature_names': dig_train_raw.columns[:-1],\n",
    "             'target_names': [str(i) for i in range(0, 10)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1725951422830,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "NMbX-NupZz2p"
   },
   "outputs": [],
   "source": [
    "dig_test_raw = pd.read_csv(os.path.join(data_dir, 'Digits_test.csv'), sep=',')\n",
    "dig_test = {'data': np.array(dig_test_raw.iloc[:, :-1]),\n",
    "            'target': np.array(dig_test_raw.iloc[:, -1]),\n",
    "            'feature_names': dig_test_raw.columns[:-1],\n",
    "            'target_names': [str(i) for i in range(0, 10)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "executionInfo": {
     "elapsed": 1815,
     "status": "ok",
     "timestamp": 1725951429065,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "Hdd_vh4uZz2p",
    "outputId": "ea81ce17-4482-43c6-c55e-59d18eefb224"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img_data(dig_train['data'].reshape((dig_train['data'].shape[0], 6, 4, 1)), figsize=(4, 4),\n",
    "              interpolation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1725951432720,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "IUlRxKWRZz2p",
    "outputId": "47986ef0-cacb-472a-dae2-850356003f49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 5, 1, 0, 6, 0],\n",
       "       [3, 2, 3, 4, 9, 5],\n",
       "       [3, 3, 1, 6, 4, 7],\n",
       "       [5, 0, 1, 0, 2, 0],\n",
       "       [2, 6, 5, 2, 7, 2]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dig_train['target'][range(0, 30)].reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1725951435412,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "lzYOx6rycIdF"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(dig_train['data'], dig_train['target'],\n",
    "                                                  test_size=0.3, random_state=4232)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIRr1M4iZz2r"
   },
   "source": [
    "## Functions for Hyperparameter/Architecture Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1725951441732,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "YJd7EuX0bLSG"
   },
   "outputs": [],
   "source": [
    "def create_hyperparams(n, default_lr=0.001):\n",
    "    df = pd.DataFrame(index=range(n),\n",
    "                      columns=['no_hidden_layers', 'hidden_layers', 'activation', 'dropout', 'lr', 'epochs'])\n",
    "\n",
    "    for i in range(n):\n",
    "        df.loc[i, 'lr'] = default_lr * 5.**random.uniform(-1., 1.)\n",
    "        df.loc[i, 'epochs'] = random.sample([64, 128, 256], 1)[0] # Removed 16 and 32 epochs ==> too few epochs. Also added 256\n",
    "\n",
    "        no_layers = random.randint(2, 6) # Zero and 1 hidden layers make no sense. Also increase to 6 hidden layers instead of 4\n",
    "        df.loc[i, 'no_hidden_layers'] = no_layers\n",
    "        df.loc[i, 'hidden_layers'] = [int(random.sample([32, 64, 128, 256, 512], 1)[0]) for i in range(no_layers)] # Removed 8 and 16 neurons ==> too few neurons. Also added 128, 256 and 512\n",
    "        df.loc[i, 'dropout'] = random.sample([0.2, 0.3, 0.4, 0.5], 1)[0] # Removed 0 dropout, because it is not recommended for deep networks\n",
    "        df.loc[i, 'activation'] = random.sample(['relu', 'elu'], 1)[0] # Removed sigmoid, because it does not train well for deep networks\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1725951444068,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "fFtzoyAFbaTf"
   },
   "outputs": [],
   "source": [
    "def create_network(hp, no_inputs, no_outputs, output_activation='softmax', **kwargs):\n",
    "    hidden_layers = hp['hidden_layers']\n",
    "\n",
    "    dropout = hp['dropout']\n",
    "    hidden_activation = hp['activation']\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(no_inputs, )))\n",
    "\n",
    "    for cl in hidden_layers:\n",
    "        model.add(tf.keras.layers.Dense(cl, activation=hidden_activation))\n",
    "        if dropout > 0:\n",
    "            model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(no_outputs, activation=output_activation))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=hp['lr'])\n",
    "\n",
    "    model.compile(optimizer=opt, **kwargs)\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1725951447574,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "3qoRsDnIyhQ-",
    "outputId": "6c864419-8dbc-4472-d235-b97bb5781ac1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>[512, 512, 128, 256, 512, 512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>[32, 256, 256, 256, 128, 128]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[64, 64, 256]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>[32, 128, 128, 64, 512, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>[512, 32, 32, 32, 128, 128]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>3</td>\n",
       "      <td>[256, 64, 512]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>6</td>\n",
       "      <td>[512, 512, 64, 64, 128, 512]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>6</td>\n",
       "      <td>[64, 32, 256, 128, 64, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 32, 32]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>4</td>\n",
       "      <td>[256, 64, 256, 32]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_hidden_layers                   hidden_layers activation dropout  \\\n",
       "0                   6  [512, 512, 128, 256, 512, 512]       relu     0.3   \n",
       "1                   6   [32, 256, 256, 256, 128, 128]        elu     0.3   \n",
       "2                   3                   [64, 64, 256]        elu     0.2   \n",
       "3                   6    [32, 128, 128, 64, 512, 256]       relu     0.2   \n",
       "4                   6     [512, 32, 32, 32, 128, 128]       relu     0.3   \n",
       "...               ...                             ...        ...     ...   \n",
       "2995                3                  [256, 64, 512]        elu     0.3   \n",
       "2996                6    [512, 512, 64, 64, 128, 512]        elu     0.2   \n",
       "2997                6     [64, 32, 256, 128, 64, 256]       relu     0.5   \n",
       "2998                3                   [512, 32, 32]        elu     0.5   \n",
       "2999                4              [256, 64, 256, 32]        elu     0.2   \n",
       "\n",
       "            lr epochs  \n",
       "0     0.001206     64  \n",
       "1     0.004129    256  \n",
       "2     0.001042    256  \n",
       "3     0.000629    128  \n",
       "4     0.000205     64  \n",
       "...        ...    ...  \n",
       "2995  0.001433    128  \n",
       "2996  0.000216     64  \n",
       "2997  0.000721     64  \n",
       "2998  0.001211    128  \n",
       "2999  0.001111     64  \n",
       "\n",
       "[3000 rows x 6 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_hyperparams(100000) # Create 100000 hyperparameters to search through\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 1441,
     "status": "ok",
     "timestamp": 1725951453572,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "3EoJWf0Obfe1"
   },
   "outputs": [],
   "source": [
    "model = create_network(df.iloc[1, :], no_inputs=24, no_outputs=10, loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1725951455805,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "2hYyghEmbiTQ",
    "outputId": "9887751a-eb2c-4a17-b3fc-53381ba39203"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m8,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">191,530</span> (748.16 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m191,530\u001b[0m (748.16 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">191,530</span> (748.16 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m191,530\u001b[0m (748.16 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1725951459383,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "KxQzPx_-bpl9"
   },
   "outputs": [],
   "source": [
    "def find_best(df, crit='ACC'):\n",
    "    index = np.where(df[crit] == np.amax(df[crit]))[0]\n",
    "    return(df.iloc[list(index), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2zsFjuDbrDW"
   },
   "source": [
    "## Perform Model Selection and Determine Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1725951469355,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "D0tU9QQKbvb8"
   },
   "outputs": [],
   "source": [
    "random.seed(4232)\n",
    "batch_size = 32\n",
    "no_models = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1725951471887,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "0_CsQAjUbzws"
   },
   "outputs": [],
   "source": [
    "model_sel = create_hyperparams(no_models)\n",
    "model_sel['ACC'] = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1725951474358,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "W7CmdoUuzMvD",
    "outputId": "3aecc9f9-9769-4bb8-9216-bc048c546691"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[64, 64, 32, 256, 32]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>128</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00256</td>\n",
       "      <td>64</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>[256, 256, 32, 32, 256, 128]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>128</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[256, 512, 32, 512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[32, 512, 128]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>[256, 256, 64, 128, 32, 256]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>[128, 64, 128, 128]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.004138</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>[128, 64, 256, 32, 512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>64</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>[256, 32, 256, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>64</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>[128, 128, 64, 128, 64, 512]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>64</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>[256, 256, 32, 32]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>64</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>[512, 64, 512]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>128</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>[256, 512, 256, 128, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>64</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>[128, 32, 512, 32]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>64</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>[64, 256, 32, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>128</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>[256, 512, 32, 128, 32, 64]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 64]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>128</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>[128, 512, 512, 256]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>[32, 32, 64, 128, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>128</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00197</td>\n",
       "      <td>64</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>[64, 128, 128, 128, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>128</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>[64, 32, 256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>64</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>[128, 256, 512]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>64</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6</td>\n",
       "      <td>[32, 256, 32, 512, 256, 512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>128</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_hidden_layers                  hidden_layers activation dropout  \\\n",
       "0                 5          [64, 64, 32, 256, 32]        elu     0.2   \n",
       "1                 2                     [512, 256]        elu     0.5   \n",
       "2                 6   [256, 256, 32, 32, 256, 128]       relu     0.5   \n",
       "3                 4            [256, 512, 32, 512]       relu     0.3   \n",
       "4                 3                 [32, 512, 128]        elu     0.4   \n",
       "5                 6   [256, 256, 64, 128, 32, 256]        elu     0.2   \n",
       "6                 4            [128, 64, 128, 128]        elu     0.5   \n",
       "7                 5        [128, 64, 256, 32, 512]       relu     0.5   \n",
       "8                 4            [256, 32, 256, 256]       relu     0.5   \n",
       "9                 6   [128, 128, 64, 128, 64, 512]        elu     0.3   \n",
       "10                4             [256, 256, 32, 32]        elu     0.3   \n",
       "11                2                     [128, 512]       relu     0.3   \n",
       "12                3                 [512, 64, 512]        elu     0.2   \n",
       "13                6  [256, 512, 256, 128, 256, 64]       relu     0.2   \n",
       "14                4             [128, 32, 512, 32]        elu     0.2   \n",
       "15                4              [64, 256, 32, 64]       relu     0.4   \n",
       "16                6    [256, 512, 32, 128, 32, 64]        elu     0.2   \n",
       "17                2                      [256, 64]        elu     0.5   \n",
       "18                4           [128, 512, 512, 256]        elu     0.4   \n",
       "19                5          [32, 32, 64, 128, 64]       relu     0.2   \n",
       "20                2                     [256, 512]       relu     0.2   \n",
       "21                5        [64, 128, 128, 128, 64]       relu     0.3   \n",
       "22                3                  [64, 32, 256]       relu     0.2   \n",
       "23                3                [128, 256, 512]        elu     0.2   \n",
       "24                6   [32, 256, 32, 512, 256, 512]       relu     0.2   \n",
       "\n",
       "          lr epochs  ACC  \n",
       "0   0.000845    128 -1.0  \n",
       "1    0.00256     64 -1.0  \n",
       "2   0.000638    128 -1.0  \n",
       "3   0.001695    256 -1.0  \n",
       "4    0.00191    256 -1.0  \n",
       "5   0.001705    256 -1.0  \n",
       "6   0.004138    256 -1.0  \n",
       "7   0.000233     64 -1.0  \n",
       "8   0.000447     64 -1.0  \n",
       "9   0.000794     64 -1.0  \n",
       "10  0.000391     64 -1.0  \n",
       "11  0.000385    256 -1.0  \n",
       "12  0.000374    128 -1.0  \n",
       "13  0.002647     64 -1.0  \n",
       "14  0.001965     64 -1.0  \n",
       "15  0.001645    128 -1.0  \n",
       "16  0.000328    256 -1.0  \n",
       "17  0.000935    128 -1.0  \n",
       "18  0.000996    256 -1.0  \n",
       "19  0.001032    128 -1.0  \n",
       "20   0.00197     64 -1.0  \n",
       "21  0.000465    128 -1.0  \n",
       "22  0.000502     64 -1.0  \n",
       "23  0.002057     64 -1.0  \n",
       "24  0.000929    128 -1.0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413258,
     "status": "ok",
     "timestamp": 1725951898844,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "mPYnYCYKb2Ug",
    "outputId": "fc5910bd-a6d6-46fd-b66e-affbfe56c149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m910s\u001b[0m 36s/step\n"
     ]
    }
   ],
   "source": [
    "pbar = tf.keras.utils.Progbar(target=no_models, stateful_metrics=[]) ## progress bar\n",
    "for i in range(no_models):\n",
    "    model = create_network(model_sel.iloc[i], no_inputs=X_train.shape[1],\n",
    "                           no_outputs=10, loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x=X_train, y=y_train,\n",
    "                        epochs=model_sel['epochs'][i],\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "\n",
    "    pred = model.predict(x=X_val, verbose=0)\n",
    "    predC = np.argmax(pred, axis=1)\n",
    "\n",
    "    model_sel.loc[i, 'ACC'] = accuracy_score(y_val, predC)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    pbar.update(i, finalize=False)\n",
    "pbar.update(no_models, finalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1725951938286,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "jo0QXrbPnK_N",
    "outputId": "a1fb369c-0533-446e-e169-3a12a4b1073d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>256</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00197</td>\n",
       "      <td>64</td>\n",
       "      <td>0.952222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>[256, 512, 32, 128, 32, 64]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>256</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>[256, 512, 256, 128, 256, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>64</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[256, 512, 32, 512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>256</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>[256, 256, 64, 128, 32, 256]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>256</td>\n",
       "      <td>0.943333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00256</td>\n",
       "      <td>64</td>\n",
       "      <td>0.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>[128, 512, 512, 256]</td>\n",
       "      <td>elu</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>256</td>\n",
       "      <td>0.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>[64, 128, 128, 128, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>128</td>\n",
       "      <td>0.938889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>[64, 256, 32, 64]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>128</td>\n",
       "      <td>0.938889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_hidden_layers                  hidden_layers activation dropout  \\\n",
       "11                2                     [128, 512]       relu     0.3   \n",
       "20                2                     [256, 512]       relu     0.2   \n",
       "16                6    [256, 512, 32, 128, 32, 64]        elu     0.2   \n",
       "13                6  [256, 512, 256, 128, 256, 64]       relu     0.2   \n",
       "3                 4            [256, 512, 32, 512]       relu     0.3   \n",
       "5                 6   [256, 256, 64, 128, 32, 256]        elu     0.2   \n",
       "1                 2                     [512, 256]        elu     0.5   \n",
       "18                4           [128, 512, 512, 256]        elu     0.4   \n",
       "21                5        [64, 128, 128, 128, 64]       relu     0.3   \n",
       "15                4              [64, 256, 32, 64]       relu     0.4   \n",
       "\n",
       "          lr epochs       ACC  \n",
       "11  0.000385    256  0.953333  \n",
       "20   0.00197     64  0.952222  \n",
       "16  0.000328    256  0.946667  \n",
       "13  0.002647     64  0.944444  \n",
       "3   0.001695    256  0.944444  \n",
       "5   0.001705    256  0.943333  \n",
       "1    0.00256     64  0.941111  \n",
       "18  0.000996    256  0.941111  \n",
       "21  0.000465    128  0.938889  \n",
       "15  0.001645    128  0.938889  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.sort_values(by='ACC', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1725951941761,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "oKRrMA27ciRn",
    "outputId": "874a977d-3ae1-445c-cb1a-6309d12deb79"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>[128, 512]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>256</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_hidden_layers hidden_layers activation dropout        lr epochs  \\\n",
       "11                2    [128, 512]       relu     0.3  0.000385    256   \n",
       "\n",
       "         ACC  \n",
       "11  0.953333  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best(model_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1725951945175,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "W7C3g4pHcG67"
   },
   "outputs": [],
   "source": [
    "best_index = find_best(model_sel).index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3gGoxx4cldW"
   },
   "source": [
    "## Train Model on Entire Training Set Using Best Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1725951948024,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "dHrP42SjcoGa"
   },
   "outputs": [],
   "source": [
    "model = create_network(model_sel.loc[best_index], no_inputs=X_train.shape[1],\n",
    "                       no_outputs=10, loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1725951950325,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "GevKGfVpcqUe",
    "outputId": "72696c2a-4ae3-488a-93ac-dfda5326b50d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m3,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">74,378</span> (290.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m74,378\u001b[0m (290.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">74,378</span> (290.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m74,378\u001b[0m (290.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6459,
     "status": "ok",
     "timestamp": 1725951969662,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "8SOr03A1ctap",
    "outputId": "c02a721c-e9f5-486e-c7b3-5557ec6cb564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2166 - loss: 2.2031\n",
      "Epoch 2/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6236 - loss: 1.4369\n",
      "Epoch 3/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7286 - loss: 0.9463\n",
      "Epoch 4/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7797 - loss: 0.7794\n",
      "Epoch 5/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7752 - loss: 0.7152\n",
      "Epoch 6/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8003 - loss: 0.6369\n",
      "Epoch 7/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8231 - loss: 0.5984\n",
      "Epoch 8/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8383 - loss: 0.5370\n",
      "Epoch 9/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8437 - loss: 0.5212\n",
      "Epoch 10/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8466 - loss: 0.5242\n",
      "Epoch 11/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8639 - loss: 0.4470\n",
      "Epoch 12/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8643 - loss: 0.4524\n",
      "Epoch 13/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8713 - loss: 0.4375\n",
      "Epoch 14/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8733 - loss: 0.4090\n",
      "Epoch 15/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8765 - loss: 0.4333\n",
      "Epoch 16/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8826 - loss: 0.3985\n",
      "Epoch 17/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8921 - loss: 0.3779\n",
      "Epoch 18/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8872 - loss: 0.3826\n",
      "Epoch 19/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8877 - loss: 0.3542\n",
      "Epoch 20/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8911 - loss: 0.3396\n",
      "Epoch 21/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9058 - loss: 0.3339\n",
      "Epoch 22/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8993 - loss: 0.3282\n",
      "Epoch 23/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8930 - loss: 0.3198\n",
      "Epoch 24/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9063 - loss: 0.3044\n",
      "Epoch 25/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9116 - loss: 0.2892\n",
      "Epoch 26/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9074 - loss: 0.3020\n",
      "Epoch 27/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9110 - loss: 0.3032\n",
      "Epoch 28/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9109 - loss: 0.2753\n",
      "Epoch 29/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9155 - loss: 0.2894\n",
      "Epoch 30/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9215 - loss: 0.2772\n",
      "Epoch 31/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9231 - loss: 0.2515\n",
      "Epoch 32/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9143 - loss: 0.2668\n",
      "Epoch 33/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9157 - loss: 0.2646\n",
      "Epoch 34/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9153 - loss: 0.2750\n",
      "Epoch 35/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9242 - loss: 0.2529\n",
      "Epoch 36/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9313 - loss: 0.2318\n",
      "Epoch 37/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9187 - loss: 0.2429\n",
      "Epoch 38/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9268 - loss: 0.2372\n",
      "Epoch 39/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9384 - loss: 0.2189\n",
      "Epoch 40/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9275 - loss: 0.2288\n",
      "Epoch 41/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9353 - loss: 0.2062\n",
      "Epoch 42/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9343 - loss: 0.2220\n",
      "Epoch 43/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9306 - loss: 0.2167\n",
      "Epoch 44/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9338 - loss: 0.2209\n",
      "Epoch 45/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9304 - loss: 0.2163\n",
      "Epoch 46/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9250 - loss: 0.2139\n",
      "Epoch 47/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9331 - loss: 0.2216\n",
      "Epoch 48/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9379 - loss: 0.1967\n",
      "Epoch 49/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9386 - loss: 0.1942\n",
      "Epoch 50/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9378 - loss: 0.1986\n",
      "Epoch 51/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9461 - loss: 0.1916\n",
      "Epoch 52/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9452 - loss: 0.1762\n",
      "Epoch 53/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.2093\n",
      "Epoch 54/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9525 - loss: 0.1617\n",
      "Epoch 55/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9442 - loss: 0.1685\n",
      "Epoch 56/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9387 - loss: 0.1943\n",
      "Epoch 57/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9495 - loss: 0.1734\n",
      "Epoch 58/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9369 - loss: 0.1870\n",
      "Epoch 59/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9447 - loss: 0.1642\n",
      "Epoch 60/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9390 - loss: 0.1827\n",
      "Epoch 61/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9508 - loss: 0.1648\n",
      "Epoch 62/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9477 - loss: 0.1616\n",
      "Epoch 63/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9506 - loss: 0.1595\n",
      "Epoch 64/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9567 - loss: 0.1569\n",
      "Epoch 65/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9564 - loss: 0.1476\n",
      "Epoch 66/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9517 - loss: 0.1602\n",
      "Epoch 67/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9593 - loss: 0.1391\n",
      "Epoch 68/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9503 - loss: 0.1498\n",
      "Epoch 69/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9504 - loss: 0.1541\n",
      "Epoch 70/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9598 - loss: 0.1277\n",
      "Epoch 71/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9548 - loss: 0.1526\n",
      "Epoch 72/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9534 - loss: 0.1447\n",
      "Epoch 73/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9580 - loss: 0.1494\n",
      "Epoch 74/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9535 - loss: 0.1452\n",
      "Epoch 75/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9593 - loss: 0.1213\n",
      "Epoch 76/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9462 - loss: 0.1628\n",
      "Epoch 77/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9575 - loss: 0.1347\n",
      "Epoch 78/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9543 - loss: 0.1362\n",
      "Epoch 79/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9641 - loss: 0.1221\n",
      "Epoch 80/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9623 - loss: 0.1249\n",
      "Epoch 81/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9602 - loss: 0.1353\n",
      "Epoch 82/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9618 - loss: 0.1271\n",
      "Epoch 83/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9558 - loss: 0.1408\n",
      "Epoch 84/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9576 - loss: 0.1323\n",
      "Epoch 85/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1121\n",
      "Epoch 86/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9640 - loss: 0.1137\n",
      "Epoch 87/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9637 - loss: 0.1110\n",
      "Epoch 88/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9579 - loss: 0.1265\n",
      "Epoch 89/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.1272\n",
      "Epoch 90/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9576 - loss: 0.1241\n",
      "Epoch 91/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9573 - loss: 0.1413\n",
      "Epoch 92/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9653 - loss: 0.1158\n",
      "Epoch 93/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9637 - loss: 0.1170\n",
      "Epoch 94/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9682 - loss: 0.1027\n",
      "Epoch 95/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1134\n",
      "Epoch 96/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9562 - loss: 0.1244\n",
      "Epoch 97/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.1126\n",
      "Epoch 98/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9644 - loss: 0.1070\n",
      "Epoch 99/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9603 - loss: 0.1164\n",
      "Epoch 100/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9644 - loss: 0.1013\n",
      "Epoch 101/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9627 - loss: 0.1052\n",
      "Epoch 102/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9660 - loss: 0.1019\n",
      "Epoch 103/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9699 - loss: 0.0910\n",
      "Epoch 104/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9680 - loss: 0.1061\n",
      "Epoch 105/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9675 - loss: 0.1008\n",
      "Epoch 106/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9680 - loss: 0.0942\n",
      "Epoch 107/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9712 - loss: 0.0859\n",
      "Epoch 108/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9657 - loss: 0.0960\n",
      "Epoch 109/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9690 - loss: 0.0919\n",
      "Epoch 110/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9751 - loss: 0.0901\n",
      "Epoch 111/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9685 - loss: 0.0947\n",
      "Epoch 112/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9680 - loss: 0.1040\n",
      "Epoch 113/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9643 - loss: 0.1073\n",
      "Epoch 114/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9649 - loss: 0.0956\n",
      "Epoch 115/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9648 - loss: 0.0951\n",
      "Epoch 116/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.1015\n",
      "Epoch 117/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.0977\n",
      "Epoch 118/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9714 - loss: 0.0848\n",
      "Epoch 119/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9702 - loss: 0.0870\n",
      "Epoch 120/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9813 - loss: 0.0703\n",
      "Epoch 121/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9730 - loss: 0.0736\n",
      "Epoch 122/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.0782\n",
      "Epoch 123/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9720 - loss: 0.0870\n",
      "Epoch 124/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9731 - loss: 0.0818\n",
      "Epoch 125/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9659 - loss: 0.0978\n",
      "Epoch 126/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9692 - loss: 0.0949\n",
      "Epoch 127/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9751 - loss: 0.0733\n",
      "Epoch 128/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9742 - loss: 0.0779\n",
      "Epoch 129/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9663 - loss: 0.0873\n",
      "Epoch 130/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9687 - loss: 0.0943\n",
      "Epoch 131/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9734 - loss: 0.0845\n",
      "Epoch 132/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9809 - loss: 0.0692\n",
      "Epoch 133/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9818 - loss: 0.0613\n",
      "Epoch 134/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 0.0798\n",
      "Epoch 135/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.0697\n",
      "Epoch 136/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9763 - loss: 0.0779\n",
      "Epoch 137/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9680 - loss: 0.0969\n",
      "Epoch 138/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9782 - loss: 0.0722\n",
      "Epoch 139/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9745 - loss: 0.0785\n",
      "Epoch 140/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9719 - loss: 0.0776\n",
      "Epoch 141/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.0659\n",
      "Epoch 142/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9735 - loss: 0.0746\n",
      "Epoch 143/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.0645\n",
      "Epoch 144/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0627\n",
      "Epoch 145/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.0677\n",
      "Epoch 146/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9690 - loss: 0.0805\n",
      "Epoch 147/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9787 - loss: 0.0651\n",
      "Epoch 148/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9734 - loss: 0.0720\n",
      "Epoch 149/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9763 - loss: 0.0729\n",
      "Epoch 150/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0723\n",
      "Epoch 151/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9739 - loss: 0.0759\n",
      "Epoch 152/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.0628\n",
      "Epoch 153/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9835 - loss: 0.0567\n",
      "Epoch 154/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9827 - loss: 0.0590\n",
      "Epoch 155/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0586\n",
      "Epoch 156/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0590\n",
      "Epoch 157/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.0505\n",
      "Epoch 158/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9759 - loss: 0.0654\n",
      "Epoch 159/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.0624\n",
      "Epoch 160/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.0537\n",
      "Epoch 161/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9769 - loss: 0.0721\n",
      "Epoch 162/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9818 - loss: 0.0572\n",
      "Epoch 163/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0692\n",
      "Epoch 164/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.0572\n",
      "Epoch 165/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9884 - loss: 0.0515\n",
      "Epoch 166/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9867 - loss: 0.0473\n",
      "Epoch 167/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.0625\n",
      "Epoch 168/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9839 - loss: 0.0496\n",
      "Epoch 169/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.0601\n",
      "Epoch 170/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0620\n",
      "Epoch 171/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.0634\n",
      "Epoch 172/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9798 - loss: 0.0572\n",
      "Epoch 173/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9822 - loss: 0.0531\n",
      "Epoch 174/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.0586\n",
      "Epoch 175/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0528\n",
      "Epoch 176/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0551\n",
      "Epoch 177/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0530\n",
      "Epoch 178/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0548\n",
      "Epoch 179/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0484\n",
      "Epoch 180/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9814 - loss: 0.0523\n",
      "Epoch 181/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9754 - loss: 0.0598\n",
      "Epoch 182/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9797 - loss: 0.0601\n",
      "Epoch 183/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9837 - loss: 0.0521\n",
      "Epoch 184/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9758 - loss: 0.0609\n",
      "Epoch 185/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9846 - loss: 0.0482\n",
      "Epoch 186/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9862 - loss: 0.0488\n",
      "Epoch 187/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9899 - loss: 0.0408\n",
      "Epoch 188/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0462\n",
      "Epoch 189/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9899 - loss: 0.0404\n",
      "Epoch 190/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0510\n",
      "Epoch 191/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9809 - loss: 0.0530\n",
      "Epoch 192/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9792 - loss: 0.0540\n",
      "Epoch 193/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9851 - loss: 0.0488\n",
      "Epoch 194/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0479\n",
      "Epoch 195/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0491\n",
      "Epoch 196/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.0530\n",
      "Epoch 197/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9852 - loss: 0.0472\n",
      "Epoch 198/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0382\n",
      "Epoch 199/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0418\n",
      "Epoch 200/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9825 - loss: 0.0490\n",
      "Epoch 201/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0432\n",
      "Epoch 202/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0481\n",
      "Epoch 203/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9853 - loss: 0.0452\n",
      "Epoch 204/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0411\n",
      "Epoch 205/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0403\n",
      "Epoch 206/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0560\n",
      "Epoch 207/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9846 - loss: 0.0471\n",
      "Epoch 208/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0438\n",
      "Epoch 209/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0386\n",
      "Epoch 210/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9860 - loss: 0.0418\n",
      "Epoch 211/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0521\n",
      "Epoch 212/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0428\n",
      "Epoch 213/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0389\n",
      "Epoch 214/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9818 - loss: 0.0471\n",
      "Epoch 215/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9883 - loss: 0.0452\n",
      "Epoch 216/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0459\n",
      "Epoch 217/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9895 - loss: 0.0401\n",
      "Epoch 218/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9821 - loss: 0.0445\n",
      "Epoch 219/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9870 - loss: 0.0452\n",
      "Epoch 220/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9842 - loss: 0.0410\n",
      "Epoch 221/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9868 - loss: 0.0345\n",
      "Epoch 222/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0467\n",
      "Epoch 223/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0360\n",
      "Epoch 224/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9885 - loss: 0.0409\n",
      "Epoch 225/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0359\n",
      "Epoch 226/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0336\n",
      "Epoch 227/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9891 - loss: 0.0345\n",
      "Epoch 228/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9908 - loss: 0.0356\n",
      "Epoch 229/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9890 - loss: 0.0460\n",
      "Epoch 230/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9894 - loss: 0.0311\n",
      "Epoch 231/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0389\n",
      "Epoch 232/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0391\n",
      "Epoch 233/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9855 - loss: 0.0409\n",
      "Epoch 234/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.0448\n",
      "Epoch 235/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0354\n",
      "Epoch 236/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9884 - loss: 0.0336\n",
      "Epoch 237/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9853 - loss: 0.0451\n",
      "Epoch 238/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0417\n",
      "Epoch 239/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9894 - loss: 0.0364\n",
      "Epoch 240/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9894 - loss: 0.0358\n",
      "Epoch 241/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9850 - loss: 0.0451\n",
      "Epoch 242/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0340\n",
      "Epoch 243/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9835 - loss: 0.0473\n",
      "Epoch 244/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9849 - loss: 0.0378\n",
      "Epoch 245/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9907 - loss: 0.0260\n",
      "Epoch 246/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9867 - loss: 0.0379\n",
      "Epoch 247/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.0485\n",
      "Epoch 248/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0370\n",
      "Epoch 249/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9855 - loss: 0.0404\n",
      "Epoch 250/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0302\n",
      "Epoch 251/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0340\n",
      "Epoch 252/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0336\n",
      "Epoch 253/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9883 - loss: 0.0311\n",
      "Epoch 254/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9920 - loss: 0.0283\n",
      "Epoch 255/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0372\n",
      "Epoch 256/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0345\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=dig_train['data'], y=dig_train['target'],\n",
    "                    epochs=model_sel.loc[best_index, 'epochs'],\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tovQW_Llc1Fw"
   },
   "source": [
    "## Test Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1725951975129,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "5g81CNxUc2Z8",
    "outputId": "3a1f63f5-c109-45ba-8a8d-a0e6bc3df76f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Confusion matrix (rows -> true, columns -> predicted):\n",
      "\n",
      "     0    1    2    3    4    5    6    7    8    9\n",
      "0  208    2    0    0    0    0    1    1    0    0\n",
      "1    0  196    2    0    1    0    0    1    0    3\n",
      "2    0    2  183    0    1    0    0    0    0    0\n",
      "3    2    0    2  182    1    5    0    1    1    8\n",
      "4    0    1    0    0  202    0    3    0    0    2\n",
      "5    1    0    0    1    0  181    4    0    0    3\n",
      "6    2    2    0    0    2    1  197    0    0    0\n",
      "7    0    2    5    1    0    0    0  190    1    3\n",
      "8    1    2    0    4    3    2    1    1  179    0\n",
      "9    1    0    1    2    1    2    0    0    1  196\n",
      "\n",
      "\n",
      "Class 0:\n",
      "    Sensitivity (TPR):  98.113% (208 of 212)\n",
      "    Specificity (TNR):  99.609% (1785 of 1792)\n",
      "    Precision:          96.744% (208 of 215)\n",
      "    Neg. pred. value:   99.776% (1785 of 1789)\n",
      "Class 1:\n",
      "    Sensitivity (TPR):  96.552% (196 of 203)\n",
      "    Specificity (TNR):  99.389% (1790 of 1801)\n",
      "    Precision:          94.686% (196 of 207)\n",
      "    Neg. pred. value:   99.610% (1790 of 1797)\n",
      "Class 2:\n",
      "    Sensitivity (TPR):  98.387% (183 of 186)\n",
      "    Specificity (TNR):  99.450% (1808 of 1818)\n",
      "    Precision:          94.819% (183 of 193)\n",
      "    Neg. pred. value:   99.834% (1808 of 1811)\n",
      "Class 3:\n",
      "    Sensitivity (TPR):  90.099% (182 of 202)\n",
      "    Specificity (TNR):  99.556% (1794 of 1802)\n",
      "    Precision:          95.789% (182 of 190)\n",
      "    Neg. pred. value:   98.897% (1794 of 1814)\n",
      "Class 4:\n",
      "    Sensitivity (TPR):  97.115% (202 of 208)\n",
      "    Specificity (TNR):  99.499% (1787 of 1796)\n",
      "    Precision:          95.735% (202 of 211)\n",
      "    Neg. pred. value:   99.665% (1787 of 1793)\n",
      "Class 5:\n",
      "    Sensitivity (TPR):  95.263% (181 of 190)\n",
      "    Specificity (TNR):  99.449% (1804 of 1814)\n",
      "    Precision:          94.764% (181 of 191)\n",
      "    Neg. pred. value:   99.504% (1804 of 1813)\n",
      "Class 6:\n",
      "    Sensitivity (TPR):  96.569% (197 of 204)\n",
      "    Specificity (TNR):  99.500% (1791 of 1800)\n",
      "    Precision:          95.631% (197 of 206)\n",
      "    Neg. pred. value:   99.611% (1791 of 1798)\n",
      "Class 7:\n",
      "    Sensitivity (TPR):  94.059% (190 of 202)\n",
      "    Specificity (TNR):  99.778% (1798 of 1802)\n",
      "    Precision:          97.938% (190 of 194)\n",
      "    Neg. pred. value:   99.337% (1798 of 1810)\n",
      "Class 8:\n",
      "    Sensitivity (TPR):  92.746% (179 of 193)\n",
      "    Specificity (TNR):  99.834% (1808 of 1811)\n",
      "    Precision:          98.352% (179 of 182)\n",
      "    Neg. pred. value:   99.232% (1808 of 1822)\n",
      "Class 9:\n",
      "    Sensitivity (TPR):  96.078% (196 of 204)\n",
      "    Specificity (TNR):  98.944% (1781 of 1800)\n",
      "    Precision:          91.163% (196 of 215)\n",
      "    Neg. pred. value:   99.553% (1781 of 1789)\n",
      "\n",
      "Overall accuracy:   95.509% (1914 of 2004)\n",
      "Balanced accuracy:  95.498%\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(dig_test['data'])\n",
    "\n",
    "evaluate_classification_result(dig_test['target'], pred, classes=dig_test['target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3800,
     "status": "ok",
     "timestamp": 1725952018810,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "qoGk887kBBza",
    "outputId": "d7afbc9a-927d-4a76-b024-8fce1fe3ccf9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Ex3_Grimm.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 378696 bytes to Ex3_Grimm.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html Ex3_Grimm.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "NDLeIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
