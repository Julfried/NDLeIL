{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "606q4MQJZz2U"
   },
   "source": [
    "# Model Selection for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp1ELm57Zz2a"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1725951345762,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "HDmggAMy91S3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## adapt this directory to your needs\n",
    "base_dir = '../'\n",
    "notebook_dir = os.path.join(base_dir, 'Exercise')\n",
    "data_dir = os.path.join(base_dir, 'DataSets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 4639,
     "status": "ok",
     "timestamp": 1725951415531,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "vXCty5i0Zz2b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyMLaux import show_img_data, evaluate_classification_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c9KhDatZz2o"
   },
   "source": [
    "## Load Simple Digit Recognition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 1413,
     "status": "ok",
     "timestamp": 1725951420044,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "D6TPTi1qZz2o"
   },
   "outputs": [],
   "source": [
    "dig_train_raw = pd.read_csv(os.path.join(data_dir,'Digits_training.csv'), sep=',')\n",
    "dig_train = {'data': np.array(dig_train_raw.iloc[:, :-1]),\n",
    "             'target': np.array(dig_train_raw.iloc[:, -1]),\n",
    "             'feature_names': dig_train_raw.columns[:-1],\n",
    "             'target_names': [str(i) for i in range(0, 10)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1725951422830,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "NMbX-NupZz2p"
   },
   "outputs": [],
   "source": [
    "dig_test_raw = pd.read_csv(os.path.join(data_dir, 'Digits_test.csv'), sep=',')\n",
    "dig_test = {'data': np.array(dig_test_raw.iloc[:, :-1]),\n",
    "            'target': np.array(dig_test_raw.iloc[:, -1]),\n",
    "            'feature_names': dig_test_raw.columns[:-1],\n",
    "            'target_names': [str(i) for i in range(0, 10)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "executionInfo": {
     "elapsed": 1815,
     "status": "ok",
     "timestamp": 1725951429065,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "Hdd_vh4uZz2p",
    "outputId": "ea81ce17-4482-43c6-c55e-59d18eefb224"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img_data(dig_train['data'].reshape((dig_train['data'].shape[0], 6, 4, 1)), figsize=(4, 4),\n",
    "              interpolation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1725951432720,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "IUlRxKWRZz2p",
    "outputId": "47986ef0-cacb-472a-dae2-850356003f49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 5, 1, 0, 6, 0],\n",
       "       [3, 2, 3, 4, 9, 5],\n",
       "       [3, 3, 1, 6, 4, 7],\n",
       "       [5, 0, 1, 0, 2, 0],\n",
       "       [2, 6, 5, 2, 7, 2]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dig_train['target'][range(0, 30)].reshape(5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1725951435412,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "lzYOx6rycIdF"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(dig_train['data'], dig_train['target'],\n",
    "                                                  test_size=0.3, random_state=4232)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIRr1M4iZz2r"
   },
   "source": [
    "## Functions for Hyperparameter/Architecture Selection\n",
    "\n",
    "\n",
    "In a previous run, I identfied architecture types which where able to achieve high accuracy. This was identified using this function by searching over 3000 models:\n",
    "\n",
    "```python\n",
    "def create_hyperparams(n, default_lr=0.001):\n",
    "    df = pd.DataFrame(index=range(n),\n",
    "                      columns=['no_hidden_layers', 'hidden_layers', 'activation', 'dropout', 'lr', 'epochs'])\n",
    "\n",
    "    for i in range(n):\n",
    "        df.loc[i, 'lr'] = default_lr * 5.**random.uniform(-1., 1.)\n",
    "        df.loc[i, 'epochs'] = random.sample([64, 128, 256], 1)[0] # Removed 16 and 32 epochs ==> too few epochs. Also added 256\n",
    "\n",
    "        no_layers = random.randint(2, 8) # Zero and 1 hidden layers make no sense. Also increase to 8 hidden layers instead of 4\n",
    "        df.loc[i, 'no_hidden_layers'] = no_layers\n",
    "\n",
    "        # Number of neuron in hidden layers: Remove random sampling, because it makes no sense if neuron counts are random ==> implement a decreasing pattern\n",
    "        # This value is only the initial value\n",
    "        # Create decreasing layer sizes\n",
    "        initial_size = random.sample([784, 392], 1)[0]  # Start large to capture all information ( 28x28 = 784 pixels or 28x14 = 392 pixels)\n",
    "        layer_sizes = [initial_size]\n",
    "        \n",
    "        # Calculate reduction factor\n",
    "        reduction = (initial_size / 32) ** (1.0 / (no_layers - 1))\n",
    "        \n",
    "        # Generate remaining layers with smooth size reduction\n",
    "        current_size = initial_size\n",
    "        for _ in range(no_layers - 1):\n",
    "            current_size = int(current_size / reduction)\n",
    "            current_size = max(32, min(current_size, layer_sizes[-1]))  # Ensure smooth decrease\n",
    "            layer_sizes.append(current_size)\n",
    "            \n",
    "        df.loc[i, 'hidden_layers'] = layer_sizes\n",
    "\n",
    "        # df.loc[i, 'hidden_layers'] = [int(random.sample([32, 64, 128, 256, 512], 1)[0]) for i in range(no_layers)] # \n",
    "        df.loc[i, 'dropout'] = random.sample([0.2, 0.3, 0.4, 0.5], 1)[0] # Removed 0 dropout, because it is not recommended for deep networks\n",
    "        df.loc[i, 'activation'] = random.sample(['relu', 'elu'], 1)[0] # Removed sigmoid, because it does not train well for deep networks\n",
    "\n",
    "    return(df)\n",
    "```\n",
    "\n",
    "The most promising architectures were:\n",
    "\n",
    "```python\n",
    "architecture_type = random.choice([\n",
    "            [784, 158, 32],  # Winner architecture\n",
    "            [784, 32],       # Simple but effective\n",
    "            [784, 256, 32],  # Variation of winner\n",
    "            [784, 196, 32],  # Another variation\n",
    "            [784, 128, 32],  # Third variation\n",
    "        ])\n",
    "```\n",
    "\n",
    "So now I am running the hyperparameter search again, but with the above architectures only. This should speed up the search and also increase the likelihood of finding a good model, because now I only have to vary the hyperparameters and not the architecture itself. The hyperparameter ranges were also narrowd down to the most promising values:\n",
    "* activation: relu ==> removed elu, because relu was almost always better\n",
    "* dropout: Narrowed down to 0.15 0.2, and 0.25 ==> removed 0.3 and 0.4, because they were not as good\n",
    "* epochs: Fixed to 256 ==> good balance between training time and accuracy\n",
    "* lr: Narrowed to these values 0.0004, 0.0005, 0.0006, 0.0008, 0.001, 0.0015, 0.002\n",
    "\n",
    "So now only 1000 models with the above architectures and the narrowed down hyperparameters will be trained as a last refinement step. For this lr sheduling was also added to the training process. This resulted in:\n",
    "* overall accuracy: 95.808%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1725951441732,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "YJd7EuX0bLSG"
   },
   "outputs": [],
   "source": [
    "def create_hyperparams(n, default_lr=0.001):\n",
    "    df = pd.DataFrame(index=range(n),\n",
    "                      columns=['no_hidden_layers', 'hidden_layers', 'activation', 'dropout', 'lr', 'epochs'])\n",
    "\n",
    "    for i in range(n):\n",
    "        # Focus on proven architectures\n",
    "        architecture_type = random.choice([\n",
    "            [784, 158, 32],  # Winner architecture\n",
    "            [784, 32],       # Simple but effective\n",
    "            [784, 256, 32],  # Variation of winner\n",
    "            [784, 196, 32],  # Another variation\n",
    "            [784, 128, 32],  # Third variation\n",
    "        ])\n",
    "        \n",
    "        df.loc[i, 'no_hidden_layers'] = len(architecture_type)\n",
    "        df.loc[i, 'hidden_layers'] = architecture_type\n",
    "        \n",
    "        # Fine-tuned learning rate range\n",
    "        df.loc[i, 'lr'] = random.choice([\n",
    "            0.0004, 0.0005, 0.0006, 0.0008, 0.001, 0.0015, 0.002\n",
    "        ])\n",
    "        \n",
    "        # Proven epoch counts\n",
    "        df.loc[i, 'epochs'] = 256\n",
    "        \n",
    "        # Optimal dropout range\n",
    "        df.loc[i, 'dropout'] = random.choice([0.15, 0.2, 0.25])\n",
    "        \n",
    "        # Stick with proven activation\n",
    "        df.loc[i, 'activation'] = 'relu'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1725951444068,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "fFtzoyAFbaTf"
   },
   "outputs": [],
   "source": [
    "def create_network(hp, no_inputs, no_outputs, output_activation='softmax', **kwargs):\n",
    "    hidden_layers = hp['hidden_layers']\n",
    "\n",
    "    dropout = hp['dropout']\n",
    "    hidden_activation = hp['activation']\n",
    "\n",
    "    # Add lr shedule\n",
    "    initial_learning_rate = hp['lr']\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(no_inputs, )))\n",
    "\n",
    "    for cl in hidden_layers:\n",
    "        model.add(tf.keras.layers.Dense(cl, activation=hidden_activation))\n",
    "        if dropout > 0:\n",
    "            model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(no_outputs, activation=output_activation))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(optimizer=opt, **kwargs)\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1725951447574,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "3qoRsDnIyhQ-",
    "outputId": "6c864419-8dbc-4472-d235-b97bb5781ac1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 196, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 256, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 256, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 256, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 128, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>[784, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 256, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 128, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>[784, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 128, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.002</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  no_hidden_layers   hidden_layers activation dropout      lr epochs\n",
       "0                3  [784, 196, 32]       relu     0.2  0.0015    256\n",
       "1                3  [784, 256, 32]       relu    0.25  0.0004    256\n",
       "2                3  [784, 256, 32]       relu     0.2  0.0015    256\n",
       "3                3  [784, 256, 32]       relu    0.25   0.001    256\n",
       "4                3  [784, 128, 32]       relu     0.2   0.001    256\n",
       "5                2       [784, 32]       relu     0.2  0.0005    256\n",
       "6                3  [784, 256, 32]       relu    0.25  0.0008    256\n",
       "7                3  [784, 128, 32]       relu    0.25   0.001    256\n",
       "8                2       [784, 32]       relu    0.25  0.0005    256\n",
       "9                3  [784, 128, 32]       relu    0.15   0.002    256"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_hyperparams(10) # Create 1000000 hyperparameters to search through\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 1441,
     "status": "ok",
     "timestamp": 1725951453572,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "3EoJWf0Obfe1"
   },
   "outputs": [],
   "source": [
    "model = create_network(df.iloc[1, :], no_inputs=24, no_outputs=10, loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1725951455805,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "2hYyghEmbiTQ",
    "outputId": "9887751a-eb2c-4a17-b3fc-53381ba39203"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │        \u001b[38;5;34m19,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m200,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m8,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">229,114</span> (894.98 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m229,114\u001b[0m (894.98 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">229,114</span> (894.98 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m229,114\u001b[0m (894.98 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1725951459383,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "KxQzPx_-bpl9"
   },
   "outputs": [],
   "source": [
    "def find_best(df, crit='ACC'):\n",
    "    index = np.where(df[crit] == np.amax(df[crit]))[0]\n",
    "    return(df.iloc[list(index), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2zsFjuDbrDW"
   },
   "source": [
    "## Perform Model Selection and Determine Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1725951469355,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "D0tU9QQKbvb8"
   },
   "outputs": [],
   "source": [
    "random.seed(4232)\n",
    "batch_size = 32\n",
    "no_models = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1725951471887,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "0_CsQAjUbzws"
   },
   "outputs": [],
   "source": [
    "model_sel = create_hyperparams(no_models)\n",
    "model_sel['ACC'] = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1725951474358,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "W7CmdoUuzMvD",
    "outputId": "3aecc9f9-9769-4bb8-9216-bc048c546691"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 196, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 196, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[784, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 158, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.002</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 158, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2</td>\n",
       "      <td>[784, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 158, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.001</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2</td>\n",
       "      <td>[784, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 196, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2</td>\n",
       "      <td>[784, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    no_hidden_layers   hidden_layers activation dropout      lr epochs  ACC\n",
       "0                  3  [784, 196, 32]       relu     0.2  0.0015    256 -1.0\n",
       "1                  3  [784, 196, 32]       relu    0.15  0.0015    256 -1.0\n",
       "2                  2       [784, 32]       relu     0.2  0.0004    256 -1.0\n",
       "3                  3  [784, 158, 32]       relu    0.25   0.002    256 -1.0\n",
       "4                  3  [784, 158, 32]       relu    0.15  0.0008    256 -1.0\n",
       "..               ...             ...        ...     ...     ...    ...  ...\n",
       "995                2       [784, 32]       relu    0.15  0.0005    256 -1.0\n",
       "996                3  [784, 158, 32]       relu    0.25   0.001    256 -1.0\n",
       "997                2       [784, 32]       relu    0.15  0.0006    256 -1.0\n",
       "998                3  [784, 196, 32]       relu     0.2  0.0006    256 -1.0\n",
       "999                2       [784, 32]       relu    0.25  0.0005    256 -1.0\n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413258,
     "status": "ok",
     "timestamp": 1725951898844,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "mPYnYCYKb2Ug",
    "outputId": "fc5910bd-a6d6-46fd-b66e-affbfe56c149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47162s\u001b[0m 47s/step\n"
     ]
    }
   ],
   "source": [
    "pbar = tf.keras.utils.Progbar(target=no_models, stateful_metrics=[]) ## progress bar\n",
    "for i in range(no_models):\n",
    "    model = create_network(model_sel.iloc[i], no_inputs=X_train.shape[1],\n",
    "                           no_outputs=10, loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x=X_train, y=y_train,\n",
    "                        epochs=model_sel['epochs'][i],\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "\n",
    "    pred = model.predict(x=X_val, verbose=0)\n",
    "    predC = np.argmax(pred, axis=1)\n",
    "\n",
    "    model_sel.loc[i, 'ACC'] = accuracy_score(y_val, predC)\n",
    "\n",
    "    # Use the modern API to clear th model\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    pbar.update(i, finalize=False)\n",
    "pbar.update(no_models, finalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1725951938286,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "jo0QXrbPnK_N",
    "outputId": "a1fb369c-0533-446e-e169-3a12a4b1073d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 256, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>256</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 256, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>256</td>\n",
       "      <td>0.965556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>2</td>\n",
       "      <td>[784, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>256</td>\n",
       "      <td>0.965556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 158, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>256</td>\n",
       "      <td>0.964444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 128, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>256</td>\n",
       "      <td>0.963333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 158, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>256</td>\n",
       "      <td>0.963333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>2</td>\n",
       "      <td>[784, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>256</td>\n",
       "      <td>0.963333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>2</td>\n",
       "      <td>[784, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>256</td>\n",
       "      <td>0.962222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 256, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.002</td>\n",
       "      <td>256</td>\n",
       "      <td>0.962222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 128, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>256</td>\n",
       "      <td>0.962222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    no_hidden_layers   hidden_layers activation dropout      lr epochs  \\\n",
       "696                3  [784, 256, 32]       relu    0.25  0.0006    256   \n",
       "697                3  [784, 256, 32]       relu    0.15  0.0004    256   \n",
       "241                2       [784, 32]       relu     0.2  0.0004    256   \n",
       "940                3  [784, 158, 32]       relu    0.15  0.0006    256   \n",
       "69                 3  [784, 128, 32]       relu     0.2  0.0004    256   \n",
       "488                3  [784, 158, 32]       relu    0.25  0.0008    256   \n",
       "463                2       [784, 32]       relu    0.15  0.0006    256   \n",
       "957                2       [784, 32]       relu    0.15  0.0006    256   \n",
       "455                3  [784, 256, 32]       relu     0.2   0.002    256   \n",
       "914                3  [784, 128, 32]       relu    0.25  0.0015    256   \n",
       "\n",
       "          ACC  \n",
       "696  0.966667  \n",
       "697  0.965556  \n",
       "241  0.965556  \n",
       "940  0.964444  \n",
       "69   0.963333  \n",
       "488  0.963333  \n",
       "463  0.963333  \n",
       "957  0.962222  \n",
       "455  0.962222  \n",
       "914  0.962222  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sel.sort_values(by='ACC', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1725951941761,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "oKRrMA27ciRn",
    "outputId": "874a977d-3ae1-445c-cb1a-6309d12deb79"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_hidden_layers</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>3</td>\n",
       "      <td>[784, 256, 32]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>256</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    no_hidden_layers   hidden_layers activation dropout      lr epochs  \\\n",
       "696                3  [784, 256, 32]       relu    0.25  0.0006    256   \n",
       "\n",
       "          ACC  \n",
       "696  0.966667  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best(model_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1725951945175,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "W7C3g4pHcG67"
   },
   "outputs": [],
   "source": [
    "best_index = find_best(model_sel).index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3gGoxx4cldW"
   },
   "source": [
    "## Train Model on Entire Training Set Using Best Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1725951948024,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "dHrP42SjcoGa"
   },
   "outputs": [],
   "source": [
    "model = create_network(model_sel.loc[best_index], no_inputs=X_train.shape[1],\n",
    "                       no_outputs=10, loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1725951950325,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "GevKGfVpcqUe",
    "outputId": "72696c2a-4ae3-488a-93ac-dfda5326b50d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │        \u001b[38;5;34m19,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m200,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m8,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">229,114</span> (894.98 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m229,114\u001b[0m (894.98 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">229,114</span> (894.98 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m229,114\u001b[0m (894.98 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6459,
     "status": "ok",
     "timestamp": 1725951969662,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "8SOr03A1ctap",
    "outputId": "c02a721c-e9f5-486e-c7b3-5557ec6cb564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.3275 - loss: 1.9819 \n",
      "Epoch 2/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6824 - loss: 1.0700\n",
      "Epoch 3/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7578 - loss: 0.7939\n",
      "Epoch 4/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7986 - loss: 0.6374\n",
      "Epoch 5/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8334 - loss: 0.5466\n",
      "Epoch 6/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8546 - loss: 0.4857\n",
      "Epoch 7/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8689 - loss: 0.4396\n",
      "Epoch 8/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8820 - loss: 0.4190\n",
      "Epoch 9/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8973 - loss: 0.3351\n",
      "Epoch 10/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8997 - loss: 0.3506\n",
      "Epoch 11/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9010 - loss: 0.3317\n",
      "Epoch 12/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9114 - loss: 0.3037\n",
      "Epoch 13/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9169 - loss: 0.3119\n",
      "Epoch 14/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9252 - loss: 0.2700\n",
      "Epoch 15/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 0.2459\n",
      "Epoch 16/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9424 - loss: 0.2238\n",
      "Epoch 17/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9225 - loss: 0.2522\n",
      "Epoch 18/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9284 - loss: 0.2371\n",
      "Epoch 19/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9262 - loss: 0.2458\n",
      "Epoch 20/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9348 - loss: 0.2168\n",
      "Epoch 21/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9325 - loss: 0.1957\n",
      "Epoch 22/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9424 - loss: 0.1920\n",
      "Epoch 23/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9440 - loss: 0.1886\n",
      "Epoch 24/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9476 - loss: 0.1894\n",
      "Epoch 25/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9432 - loss: 0.1853\n",
      "Epoch 26/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9540 - loss: 0.1565\n",
      "Epoch 27/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9585 - loss: 0.1532\n",
      "Epoch 28/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9616 - loss: 0.1346\n",
      "Epoch 29/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9623 - loss: 0.1278\n",
      "Epoch 30/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9550 - loss: 0.1444\n",
      "Epoch 31/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9584 - loss: 0.1326\n",
      "Epoch 32/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9525 - loss: 0.1530\n",
      "Epoch 33/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9631 - loss: 0.1245\n",
      "Epoch 34/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9588 - loss: 0.1306\n",
      "Epoch 35/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9605 - loss: 0.1146\n",
      "Epoch 36/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9647 - loss: 0.1245\n",
      "Epoch 37/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9722 - loss: 0.0912\n",
      "Epoch 38/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9625 - loss: 0.1066\n",
      "Epoch 39/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9641 - loss: 0.1038\n",
      "Epoch 40/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.1060\n",
      "Epoch 41/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9669 - loss: 0.1090\n",
      "Epoch 42/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9687 - loss: 0.0969\n",
      "Epoch 43/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9687 - loss: 0.1008\n",
      "Epoch 44/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9668 - loss: 0.0901\n",
      "Epoch 45/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9679 - loss: 0.0937\n",
      "Epoch 46/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9730 - loss: 0.0835\n",
      "Epoch 47/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9720 - loss: 0.0877\n",
      "Epoch 48/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0897\n",
      "Epoch 49/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9732 - loss: 0.0828\n",
      "Epoch 50/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9809 - loss: 0.0731\n",
      "Epoch 51/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9758 - loss: 0.0776\n",
      "Epoch 52/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9809 - loss: 0.0638\n",
      "Epoch 53/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9755 - loss: 0.0751\n",
      "Epoch 54/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0603\n",
      "Epoch 55/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9758 - loss: 0.0701\n",
      "Epoch 56/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9744 - loss: 0.0734\n",
      "Epoch 57/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9818 - loss: 0.0602\n",
      "Epoch 58/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9794 - loss: 0.0678\n",
      "Epoch 59/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.0697\n",
      "Epoch 60/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9797 - loss: 0.0567\n",
      "Epoch 61/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9792 - loss: 0.0796\n",
      "Epoch 62/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9747 - loss: 0.0636\n",
      "Epoch 63/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0718\n",
      "Epoch 64/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.0594\n",
      "Epoch 65/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.0626\n",
      "Epoch 66/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9821 - loss: 0.0509\n",
      "Epoch 67/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9872 - loss: 0.0496\n",
      "Epoch 68/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9834 - loss: 0.0599\n",
      "Epoch 69/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0491\n",
      "Epoch 70/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.0615\n",
      "Epoch 71/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9785 - loss: 0.0558\n",
      "Epoch 72/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.0571\n",
      "Epoch 73/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9882 - loss: 0.0401\n",
      "Epoch 74/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0511\n",
      "Epoch 75/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9752 - loss: 0.0690\n",
      "Epoch 76/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0465\n",
      "Epoch 77/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9867 - loss: 0.0425\n",
      "Epoch 78/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0459\n",
      "Epoch 79/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9889 - loss: 0.0357\n",
      "Epoch 80/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0505\n",
      "Epoch 81/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0476\n",
      "Epoch 82/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0632\n",
      "Epoch 83/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9860 - loss: 0.0470\n",
      "Epoch 84/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9853 - loss: 0.0412\n",
      "Epoch 85/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9842 - loss: 0.0402\n",
      "Epoch 86/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0311\n",
      "Epoch 87/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.0384\n",
      "Epoch 88/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.0360\n",
      "Epoch 89/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9844 - loss: 0.0437\n",
      "Epoch 90/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9837 - loss: 0.0460\n",
      "Epoch 91/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9890 - loss: 0.0438\n",
      "Epoch 92/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.0597\n",
      "Epoch 93/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0597\n",
      "Epoch 94/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9845 - loss: 0.0406\n",
      "Epoch 95/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0320\n",
      "Epoch 96/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0392\n",
      "Epoch 97/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9888 - loss: 0.0356\n",
      "Epoch 98/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9845 - loss: 0.0461\n",
      "Epoch 99/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0369\n",
      "Epoch 100/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9885 - loss: 0.0325\n",
      "Epoch 101/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0420\n",
      "Epoch 102/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9900 - loss: 0.0356\n",
      "Epoch 103/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9864 - loss: 0.0395\n",
      "Epoch 104/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0397\n",
      "Epoch 105/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0380\n",
      "Epoch 106/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9841 - loss: 0.0367\n",
      "Epoch 107/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0347\n",
      "Epoch 108/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9920 - loss: 0.0328\n",
      "Epoch 109/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0339\n",
      "Epoch 110/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9920 - loss: 0.0267\n",
      "Epoch 111/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9909 - loss: 0.0255\n",
      "Epoch 112/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0268\n",
      "Epoch 113/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9936 - loss: 0.0197\n",
      "Epoch 114/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9925 - loss: 0.0254\n",
      "Epoch 115/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9920 - loss: 0.0249\n",
      "Epoch 116/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0214\n",
      "Epoch 117/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9932 - loss: 0.0259\n",
      "Epoch 118/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0236\n",
      "Epoch 119/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9932 - loss: 0.0178\n",
      "Epoch 120/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0128\n",
      "Epoch 121/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0285\n",
      "Epoch 122/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9883 - loss: 0.0362\n",
      "Epoch 123/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9877 - loss: 0.0400\n",
      "Epoch 124/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0232\n",
      "Epoch 125/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9913 - loss: 0.0274\n",
      "Epoch 126/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0156\n",
      "Epoch 127/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0310\n",
      "Epoch 128/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.0629\n",
      "Epoch 129/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9910 - loss: 0.0388\n",
      "Epoch 130/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9932 - loss: 0.0294\n",
      "Epoch 131/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9918 - loss: 0.0272\n",
      "Epoch 132/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9937 - loss: 0.0225\n",
      "Epoch 133/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9926 - loss: 0.0248\n",
      "Epoch 134/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0243\n",
      "Epoch 135/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9973 - loss: 0.0141\n",
      "Epoch 136/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0126\n",
      "Epoch 137/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0222\n",
      "Epoch 138/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9885 - loss: 0.0363\n",
      "Epoch 139/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0198\n",
      "Epoch 140/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9901 - loss: 0.0248\n",
      "Epoch 141/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9910 - loss: 0.0243\n",
      "Epoch 142/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0221\n",
      "Epoch 143/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9936 - loss: 0.0248\n",
      "Epoch 144/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9929 - loss: 0.0217\n",
      "Epoch 145/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0245\n",
      "Epoch 146/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9925 - loss: 0.0255\n",
      "Epoch 147/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9931 - loss: 0.0288\n",
      "Epoch 148/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9942 - loss: 0.0212\n",
      "Epoch 149/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9831 - loss: 0.0615\n",
      "Epoch 150/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9899 - loss: 0.0334\n",
      "Epoch 151/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9943 - loss: 0.0200\n",
      "Epoch 152/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9955 - loss: 0.0165\n",
      "Epoch 153/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9929 - loss: 0.0208\n",
      "Epoch 154/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9948 - loss: 0.0234\n",
      "Epoch 155/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9922 - loss: 0.0240\n",
      "Epoch 156/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0152\n",
      "Epoch 157/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9909 - loss: 0.0261\n",
      "Epoch 158/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0254\n",
      "Epoch 159/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0297\n",
      "Epoch 160/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9896 - loss: 0.0246\n",
      "Epoch 161/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0165\n",
      "Epoch 162/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9940 - loss: 0.0188\n",
      "Epoch 163/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.0171\n",
      "Epoch 164/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9931 - loss: 0.0240\n",
      "Epoch 165/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0298\n",
      "Epoch 166/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0203\n",
      "Epoch 167/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0214\n",
      "Epoch 168/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0206\n",
      "Epoch 169/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0202\n",
      "Epoch 170/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0270\n",
      "Epoch 171/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0260\n",
      "Epoch 172/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0126\n",
      "Epoch 173/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9955 - loss: 0.0188\n",
      "Epoch 174/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0171\n",
      "Epoch 175/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9961 - loss: 0.0175\n",
      "Epoch 176/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9957 - loss: 0.0202\n",
      "Epoch 177/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0215\n",
      "Epoch 178/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9926 - loss: 0.0257\n",
      "Epoch 179/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9953 - loss: 0.0174\n",
      "Epoch 180/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9947 - loss: 0.0167\n",
      "Epoch 181/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0128\n",
      "Epoch 182/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0315\n",
      "Epoch 183/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9967 - loss: 0.0113\n",
      "Epoch 184/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0236\n",
      "Epoch 185/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9883 - loss: 0.0273\n",
      "Epoch 186/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9931 - loss: 0.0183\n",
      "Epoch 187/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0105\n",
      "Epoch 188/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9970 - loss: 0.0120\n",
      "Epoch 189/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9964 - loss: 0.0140\n",
      "Epoch 190/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0184\n",
      "Epoch 191/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9948 - loss: 0.0175\n",
      "Epoch 192/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0283\n",
      "Epoch 193/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9947 - loss: 0.0180\n",
      "Epoch 194/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9949 - loss: 0.0161\n",
      "Epoch 195/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0238\n",
      "Epoch 196/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0104\n",
      "Epoch 197/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9943 - loss: 0.0180\n",
      "Epoch 198/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0260\n",
      "Epoch 199/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0175\n",
      "Epoch 200/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0221\n",
      "Epoch 201/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0179\n",
      "Epoch 202/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0181\n",
      "Epoch 203/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9952 - loss: 0.0147\n",
      "Epoch 204/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9973 - loss: 0.0121\n",
      "Epoch 205/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9955 - loss: 0.0210\n",
      "Epoch 206/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9947 - loss: 0.0187\n",
      "Epoch 207/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9880 - loss: 0.0400\n",
      "Epoch 208/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9961 - loss: 0.0104\n",
      "Epoch 209/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9893 - loss: 0.0409\n",
      "Epoch 210/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0291\n",
      "Epoch 211/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9937 - loss: 0.0185\n",
      "Epoch 212/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9970 - loss: 0.0102\n",
      "Epoch 213/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 0.0248\n",
      "Epoch 214/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9949 - loss: 0.0179\n",
      "Epoch 215/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0128\n",
      "Epoch 216/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9937 - loss: 0.0224\n",
      "Epoch 217/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0132\n",
      "Epoch 218/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9964 - loss: 0.0167\n",
      "Epoch 219/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0114\n",
      "Epoch 220/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9943 - loss: 0.0208\n",
      "Epoch 221/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9943 - loss: 0.0193\n",
      "Epoch 222/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0119\n",
      "Epoch 223/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9977 - loss: 0.0064\n",
      "Epoch 224/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9967 - loss: 0.0076\n",
      "Epoch 225/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9949 - loss: 0.0157\n",
      "Epoch 226/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0187\n",
      "Epoch 227/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0160\n",
      "Epoch 228/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0149\n",
      "Epoch 229/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0091\n",
      "Epoch 230/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0091\n",
      "Epoch 231/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0075\n",
      "Epoch 232/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0168\n",
      "Epoch 233/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0056\n",
      "Epoch 234/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9929 - loss: 0.0159\n",
      "Epoch 235/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9953 - loss: 0.0187\n",
      "Epoch 236/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9962 - loss: 0.0105\n",
      "Epoch 237/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0104\n",
      "Epoch 238/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9955 - loss: 0.0153\n",
      "Epoch 239/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9955 - loss: 0.0140\n",
      "Epoch 240/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0254\n",
      "Epoch 241/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9937 - loss: 0.0210\n",
      "Epoch 242/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9931 - loss: 0.0247\n",
      "Epoch 243/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9925 - loss: 0.0295\n",
      "Epoch 244/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0110\n",
      "Epoch 245/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0103\n",
      "Epoch 246/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9965 - loss: 0.0179\n",
      "Epoch 247/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9977 - loss: 0.0084\n",
      "Epoch 248/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9968 - loss: 0.0101\n",
      "Epoch 249/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9969 - loss: 0.0203\n",
      "Epoch 250/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9955 - loss: 0.0176\n",
      "Epoch 251/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9927 - loss: 0.0247\n",
      "Epoch 252/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9961 - loss: 0.0152\n",
      "Epoch 253/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0105\n",
      "Epoch 254/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9975 - loss: 0.0115\n",
      "Epoch 255/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0176\n",
      "Epoch 256/256\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9966 - loss: 0.0092\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=dig_train['data'], y=dig_train['target'],\n",
    "                    epochs=model_sel.loc[best_index, 'epochs'],\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tovQW_Llc1Fw"
   },
   "source": [
    "## Test Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1725951975129,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "5g81CNxUc2Z8",
    "outputId": "3a1f63f5-c109-45ba-8a8d-a0e6bc3df76f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Confusion matrix (rows -> true, columns -> predicted):\n",
      "\n",
      "     0    1    2    3    4    5    6    7    8    9\n",
      "0  207    3    0    0    2    0    0    0    0    0\n",
      "1    0  198    0    0    0    0    0    1    1    3\n",
      "2    0    0  183    0    1    0    0    1    0    1\n",
      "3    1    1    0  193    0    1    0    2    0    4\n",
      "4    0    3    0    0  201    0    1    0    0    3\n",
      "5    0    1    0    4    0  178    5    0    0    2\n",
      "6    2    2    0    0    5    5  189    0    1    0\n",
      "7    0    2    1    0    0    0    0  195    1    3\n",
      "8    2    3    0    4    0    0    2    1  179    2\n",
      "9    0    1    0    3    1    1    0    0    1  197\n",
      "\n",
      "\n",
      "Class 0:\n",
      "    Sensitivity (TPR):  97.642% (207 of 212)\n",
      "    Specificity (TNR):  99.721% (1787 of 1792)\n",
      "    Precision:          97.642% (207 of 212)\n",
      "    Neg. pred. value:   99.721% (1787 of 1792)\n",
      "Class 1:\n",
      "    Sensitivity (TPR):  97.537% (198 of 203)\n",
      "    Specificity (TNR):  99.112% (1785 of 1801)\n",
      "    Precision:          92.523% (198 of 214)\n",
      "    Neg. pred. value:   99.721% (1785 of 1790)\n",
      "Class 2:\n",
      "    Sensitivity (TPR):  98.387% (183 of 186)\n",
      "    Specificity (TNR):  99.945% (1817 of 1818)\n",
      "    Precision:          99.457% (183 of 184)\n",
      "    Neg. pred. value:   99.835% (1817 of 1820)\n",
      "Class 3:\n",
      "    Sensitivity (TPR):  95.545% (193 of 202)\n",
      "    Specificity (TNR):  99.390% (1791 of 1802)\n",
      "    Precision:          94.608% (193 of 204)\n",
      "    Neg. pred. value:   99.500% (1791 of 1800)\n",
      "Class 4:\n",
      "    Sensitivity (TPR):  96.635% (201 of 208)\n",
      "    Specificity (TNR):  99.499% (1787 of 1796)\n",
      "    Precision:          95.714% (201 of 210)\n",
      "    Neg. pred. value:   99.610% (1787 of 1794)\n",
      "Class 5:\n",
      "    Sensitivity (TPR):  93.684% (178 of 190)\n",
      "    Specificity (TNR):  99.614% (1807 of 1814)\n",
      "    Precision:          96.216% (178 of 185)\n",
      "    Neg. pred. value:   99.340% (1807 of 1819)\n",
      "Class 6:\n",
      "    Sensitivity (TPR):  92.647% (189 of 204)\n",
      "    Specificity (TNR):  99.556% (1792 of 1800)\n",
      "    Precision:          95.939% (189 of 197)\n",
      "    Neg. pred. value:   99.170% (1792 of 1807)\n",
      "Class 7:\n",
      "    Sensitivity (TPR):  96.535% (195 of 202)\n",
      "    Specificity (TNR):  99.723% (1797 of 1802)\n",
      "    Precision:          97.500% (195 of 200)\n",
      "    Neg. pred. value:   99.612% (1797 of 1804)\n",
      "Class 8:\n",
      "    Sensitivity (TPR):  92.746% (179 of 193)\n",
      "    Specificity (TNR):  99.779% (1807 of 1811)\n",
      "    Precision:          97.814% (179 of 183)\n",
      "    Neg. pred. value:   99.231% (1807 of 1821)\n",
      "Class 9:\n",
      "    Sensitivity (TPR):  96.569% (197 of 204)\n",
      "    Specificity (TNR):  99.000% (1782 of 1800)\n",
      "    Precision:          91.628% (197 of 215)\n",
      "    Neg. pred. value:   99.609% (1782 of 1789)\n",
      "\n",
      "Overall accuracy:   95.808% (1920 of 2004)\n",
      "Balanced accuracy:  95.793%\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(dig_test['data'])\n",
    "\n",
    "evaluate_classification_result(dig_test['target'], pred, classes=dig_test['target_names'])\n",
    "\n",
    "# Save the model\n",
    "model.save(os.path.join(base_dir, 'Ex3_Grimm.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3800,
     "status": "ok",
     "timestamp": 1725952018810,
     "user": {
      "displayName": "Ulrich Bodenhofer",
      "userId": "07009582510819085803"
     },
     "user_tz": -120
    },
    "id": "qoGk887kBBza",
    "outputId": "d7afbc9a-927d-4a76-b024-8fce1fe3ccf9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Ex3_Grimm.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 408572 bytes to Ex3_Grimm.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html Ex3_Grimm.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "NDLeIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
